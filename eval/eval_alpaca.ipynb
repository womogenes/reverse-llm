{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978c1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "model_name = \"reverse-gpt2-0.35B-fineweb-10BT-ctx-1024-chat-v2\"\n",
    "\n",
    "MODEL_DIR = \"/home/wyf/orcd/pool/reverse-llm/models\"\n",
    "TOKENIZER_DIR = \"/home/wyf/orcd/pool/reverse-llm/tokenizers\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"{MODEL_DIR}/{model_name}/checkpoint-105\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28387677",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ROLE_NAME = \"user\"[::-1]\n",
    "ASSISTANT_ROLE_NAME = \"assistant\"[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f1e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_bpe_200k\")\n",
    "tokenizer.add_special_tokens({ \"additional_special_tokens\": [\"<im_start>\", \"<im_end>\"] })\n",
    "\n",
    "tokenizer.chat_template = \"\"\"{% for message in messages -%}\n",
    "<im_start>{{ message['role'] }}\n",
    "{{ message['content'] }}<im_end>\n",
    "{%- endfor -%}\n",
    "{% if add_generation_prompt and messages[-1]['role'] != 'assistant' -%}\n",
    "<im_start>tnatsissa{{ '\\n' }}\n",
    "{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848eddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    num_return_sequences=1,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    # bos_token_id=tokenizer.bos_token_id,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7c07ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': ['<im_start>', '<im_end>']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7981ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [52001], 'token_type_ids': [0], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<im_end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69a219ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: <im_start>resu\n",
      "ma i<im_end><im_start>tnatsissa\n",
      "\n",
      "=== RAW OUTPUT TEXT ===\n",
      "<im_start>resu\n",
      "ma i<im_end><im_start>tnatsissa\n",
      ".suoiretsym dna suoiretsym gnihtemos htiw gnilaed m'I nehw dnim ym hserfer ot ecnahc a rof gnikool m'I\n",
      "\n",
      "=== QUERY ===\n",
      "i am\n",
      "\n",
      "=== RESPONSE ===\n",
      "I'm looking for a chance to refresh my mind when I'm dealing with something mysterious and mysterious.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "   \"text-generation\",\n",
    "   model=model,\n",
    "   tokenizer=tokenizer,\n",
    "   clean_up_tokenization_spaces=False,\n",
    ")\n",
    "\n",
    "query = \"i am\"\n",
    "\n",
    "messages = [\n",
    "   {\"role\": USER_ROLE_NAME, \"content\": query[::-1]},\n",
    "]\n",
    "# For generation, apply template with add_generation_prompt=True\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"Tokenized prompt: {prompt}\")\n",
    "\n",
    "outputs = pipe(\n",
    "   prompt,\n",
    "   num_return_sequences=1,\n",
    "   # eos_token_id=tokenizer.eos_token_id,\n",
    "   max_new_tokens=128,\n",
    "   eos_token_id=52001,\n",
    "   pad_token_id=tokenizer.pad_token_id,\n",
    "   repetition_penalty=1.1,\n",
    "   temperature=0.5,\n",
    ")\n",
    "out_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "print(f\"=== RAW OUTPUT TEXT ===\\n{out_text}\")\n",
    "\n",
    "response = out_text.split(\"<im_start>tnatsissa\")[1]\n",
    "\n",
    "print(f\"\\n=== QUERY ===\\n{query}\\n\")\n",
    "print(f\"=== RESPONSE ===\\n{response[::-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
