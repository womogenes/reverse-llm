{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dc6362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available checkpoints:\n",
      "checkpoint-3500\n",
      "checkpoint-4000\n",
      "checkpoint-4500\n",
      "checkpoint-5000\n",
      "checkpoint-5500\n",
      "checkpoint-6000\n",
      "checkpoint-6500\n",
      "checkpoint-7000\n",
      "checkpoint-7500\n",
      "checkpoint-8000\n",
      "checkpoint-8500\n",
      "checkpoint-9000\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoints\n",
    "\n",
    "MODEL_DIR = \"/home/wyf/orcd/pool/reverse-llm/models\"\n",
    "TOKENIZER_DIR = \"/home/wyf/orcd/pool/reverse-llm/tokenizers\"\n",
    "DATA_DIR = \"/home/wyf/orcd/pool/reverse-llm/data\"\n",
    "\n",
    "model_name = \"reverse-gpt2-0.35B-fineweb-10BT-ctx-1024\"\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "avail_checkpoints = sorted(os.listdir(f\"{MODEL_DIR}/{model_name}\"))\n",
    "print(\"Available checkpoints:\")\n",
    "print(\"\\n\".join(avail_checkpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ed3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4614, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "val_dataset = Dataset.load_from_disk(f\"{DATA_DIR}/fineweb-10BT/tokenized_1024_valid\")\n",
    "print(val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0178255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "  3%|â–Ž         | 5/145 [00:03<01:46,  1.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m avail_checkpoints:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(checkpoint, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mget_loss\u001b[39m\u001b[34m(checkpoint)\u001b[39m\n\u001b[32m     16\u001b[39m         batch = batch.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m         loss = model(batch, labels=batch).loss\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_loss(checkpoint):\n",
    "    model_dir = f\"{MODEL_DIR}/{model_name}/{checkpoint}\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor once\n",
    "    val_tensor = torch.tensor(val_dataset['input_ids']).long()\n",
    "    dataloader = DataLoader(val_tensor, batch_size=32, shuffle=False)\n",
    "    \n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.to(\"cuda\")\n",
    "            loss = model(batch, labels=batch).loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "for checkpoint in avail_checkpoints:\n",
    "    print(checkpoint, \"\\t\", get_loss(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfea5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=f\"{TOKENIZER_DIR}/fineweb_bpe_200k.json\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "# Load model\n",
    "def generate_sample(checkpoint_no, input_text, **pipe_kwargs):\n",
    "    model = GPT2LMHeadModel.from_pretrained(f\"{MODEL_DIR}/{model_name}/checkpoint-{checkpoint_no}\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        **pipe_kwargs,\n",
    "    )\n",
    "    text = pipe(input_text)[0][\"generated_text\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cdde9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that a special character does not appear in the beginning of the book.\n",
      "The character that appears in the beginning of the book shows that a special character appears in the beginning of the book. The character that appears in the beginning of the book shows that there is a special character in this book.\n",
      "The character that appears in the beginning of the book shows that there is a special character in the book. The character that appears in the beginning of the book also shows that there is a special character. It also shows that there is a special character in this book. There is a special character in the book, which can be found near the end of the book.\n",
      "* Harry Potter is a special character in this book. The character that appears in the beginning of the novel appears in this book, but it also appears in a later chapter of his life. This not only proves to have an interesting and interesting character, but it also shows that he had had some success. Harry Potter must have had some success at some time in his life. He deserves some credit for his success until near the end of his life. After all, he was proud of his claim to fame, both for life in the 1930s, and for helping to change the world.\n",
      "* Harry Potter is the author of Harry Potter.\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(9000, \"is the author of Harry Potter.\"[::-1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d00d2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 289 tokens\n",
      "EOS in output: True\n",
      "Stopped because: EOS generated\n"
     ]
    }
   ],
   "source": [
    "# Test if model can generate EOS tokens at all\n",
    "def test_eos_generation(checkpoint_no):\n",
    "    model = GPT2LMHeadModel.from_pretrained(f\"{MODEL_DIR}/{model_name}/checkpoint-{checkpoint_no}\")\n",
    "    model.to(\"cuda\")\n",
    "    \n",
    "    input_ids = tokenizer.encode(\"The quick brown fox\"[::-1], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate with explicit EOS stopping\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    new_tokens = output[0][len(input_ids[0]):]\n",
    "    print(f\"Generated {len(new_tokens)} tokens\")\n",
    "    print(f\"EOS in output: {tokenizer.eos_token_id in new_tokens}\")\n",
    "    print(f\"Stopped because: {'EOS generated' if tokenizer.eos_token_id in new_tokens else 'Hit max_new_tokens'}\")\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "tokens = test_eos_generation(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80dcdb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound familiar? Well, we are owls!\n",
      "As you can see, owls are a very large and diverse group of birds and are composed of only about 1,800 species of owls. In fact, they are found in a large part of South America with a range ranging all the way to North America and Europe. They are responsible for creating some of the most beautiful landscapes across the world. Most areas of the United States are home to a huge number of interesting birds and animals. In this article, we'll learn more about different types of birds and owls.\n",
      "There are around 100 distinct owl species. Because of this, it is almost impossible to identify these owl species especially during the late spring and early winter. Here is some basic information about them.\n",
      "- This owl can be identified by the thick long tail feathers and a long tail. This type of colour is used to mark their territory which is of interest to many of these birds. They are also active during the day or during the night.\n",
      "- This owl can be seen throughout the season.\n",
      "- They are active during the day. They are generally nocturnal, especially at night. Due to this, they will have to leave their nests and nesting areas during the winter.\n",
      "- This area is home to several other owls such as cardinals, waxbills, leatherbills and goldfinches. These birds can also be spotted during breeding season.\n",
      "- \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[:-1])[::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
