{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a4884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"/home/wyf/orcd/pool/reverse-llm/data\"\n",
    "TOKENIZER_DIR = \"/home/wyf/orcd/pool/reverse-llm/tokenizers\"\n",
    "MODEL_DIR = \"/home/wyf/orcd/pool/reverse-llm/models\"\n",
    "\n",
    "model_name = f\"reverse-gpt2-0.35B-fineweb-10BT-ctx-1024\"\n",
    "\n",
    "USER_ROLE_NAME = \"user\"[::-1]\n",
    "ASSISTANT_ROLE_NAME = \"assistant\"[::-1]\n",
    "\n",
    "dataset_name = \"alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95e8bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_bpe_200k\")\n",
    "tokenizer.add_special_tokens({ \"additional_special_tokens\": [\"<im_start>\", \"<im_end>\"] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee7930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "raw_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "split_datasets = raw_dataset.train_test_split(test_size=0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e355b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 46801\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 5201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff9ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_alapca_data(ds_split: Dataset):\n",
    "    convos = []\n",
    "    for ex in ds_split:\n",
    "        instr = ex.get(\"instruction\", \"\").strip()[::-1]\n",
    "        ctx = ex.get(\"input\", \"\").strip()[::-1]\n",
    "        response = ex.get(\"output\", \"\").strip()[::-1]\n",
    "\n",
    "        user_msg_parts = [instr]\n",
    "        if ctx != \"\":\n",
    "            user_msg_parts.append(ctx)\n",
    "        user_msg = \"\\n\\n\".join(user_msg_parts)\n",
    "\n",
    "        if user_msg and response:\n",
    "            convos.append([\n",
    "                {\"role\": USER_ROLE_NAME, \"content\": user_msg},\n",
    "                {\"role\": ASSISTANT_ROLE_NAME, \"content\": response},\n",
    "            ])\n",
    "\n",
    "    return convos\n",
    "\n",
    "processed_alpaca = {\n",
    "    \"train\": process_alapca_data(split_datasets[\"train\"]),\n",
    "    \"valid\": process_alapca_data(split_datasets[\"test\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193a0c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Write a request for further information',\n",
       " 'input': '',\n",
       " 'output': 'Could you please provide me with additional information that could help me understand this better?',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a request for further information\\n\\n### Response:\\nCould you please provide me with additional information that could help me understand this better?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81af278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'resu', 'content': '.si eert noisiced a tahw nialpxE'}, {'role': 'tnatsissa', 'content': '.snoisiced demrofni ekam ot redro ni semoctuo tciderp dna etaulave ot desu si eert noisiced ehT .eert eht fo dne eht yfingis sevael eht dna ,noisiced nevig a no desab neppah nac taht semoctuo elbissop eht tneserper sedon eht morf sehcnarb ehT .tniop noisiced a tneserper sedon eseht dna ,sehcnarb eht ni noitcesretni na si eert eht ni edon hcaE .emoctuo ro noisiced laitnetop a gniniatnoc hcae ,sedon dne erom ro eno ot edon toor elgnis a morf tuo sehcnarb eert ehT .ssecorp gnikam-noisiced eht fo pam lausiv dezinagro na gnitaerc yb snoitpo elpitlum neewteb esoohc elpoep pleh ot desu loot a si tI .snoitidnoc niatrec no desab ,melborp a ot snoitulos elbissop fo noitatneserper lacihparg a si eert noisiced A'}]\n"
     ]
    }
   ],
   "source": [
    "print(processed_alpaca[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27dbb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<im_start>resu\n",
      ".si eert noisiced a tahw nialpxE<im_end><im_start>tnatsissa\n",
      ".snoisiced demrofni ekam ot redro ni semoctuo tciderp dna etaulave ot desu si eert noisiced ehT .eert eht fo dne eht yfingis sevael eht dna ,noisiced nevig a no desab neppah nac taht semoctuo elbissop eht tneserper sedon eht morf sehcnarb ehT .tniop noisiced a tneserper sedon eseht dna ,sehcnarb eht ni noitcesretni na si eert eht ni edon hcaE .emoctuo ro noisiced laitnetop a gniniatnoc hcae ,sedon dne erom ro eno ot edon toor elgnis a morf tuo sehcnarb eert ehT .ssecorp gnikam-noisiced eht fo pam lausiv dezinagro na gnitaerc yb snoitpo elpitlum neewteb esoohc elpoep pleh ot desu loot a si tI .snoitidnoc niatrec no desab ,melborp a ot snoitulos elbissop fo noitatneserper lacihparg a si eert noisiced A<im_end>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = \"\"\"{% for message in messages -%}\n",
    "<im_start>{{ message['role'] }}\n",
    "{{ message['content'] }}<im_end>\n",
    "{%- endfor -%}\n",
    "{% if add_generation_prompt and messages[-1]['role'] != 'assistant' -%}\n",
    "<im_start>assistant\n",
    "{%- endif %}\"\"\"\n",
    "\n",
    "print(tokenizer.decode(tokenizer.apply_chat_template(processed_alpaca[\"train\"][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4723172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering long conversations: 100%|██████████| 46775/46775 [00:07<00:00, 6491.64it/s]\n",
      "Filtering long conversations: 100%|██████████| 5199/5199 [00:00<00:00, 6430.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "context_length = 1024\n",
    "\n",
    "def filter_and_prepare_conversations(convos, tokenizer, max_len):\n",
    "    filtered_convos = []\n",
    "    for convo in tqdm(convos, desc=\"Filtering long conversations\"):\n",
    "        if not convo:\n",
    "\t        continue\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        tokenized_len = len(tokenizer.encode(prompt_text, truncation=False))\n",
    "\n",
    "        if tokenized_len > 0 and tokenized_len <= max_len:\n",
    "            filtered_convos.append(convo)\n",
    "        elif tokenized_len == 0:\n",
    "            print(f\"Zero length: {convo}\")\n",
    "        \n",
    "    return { \"conversations\": filtered_convos }\n",
    "\n",
    "\n",
    "filtered_convos = {\n",
    "    \"train\": filter_and_prepare_conversations(processed_alpaca[\"train\"], tokenizer, context_length),\n",
    "    \"valid\": filter_and_prepare_conversations(processed_alpaca[\"valid\"], tokenizer, context_length),\n",
    "}\n",
    "print(len(filtered_convos[\"train\"][\"conversations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687128b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3199b4941fb459d831005b38d3536ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39064d8191a04dadbdb2774159e5745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/46774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc95e57d0024a4eb82fa476bdefd20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfb0a90b1b34df38c17bd4528721a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    # 'example' here is something like {\"conversations\": [{\"role\": ..., \"content\": ...}, ...]}\n",
    "    conversation = example[\"conversations\"]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        truncation=True, # not really needed here bcs we already remove those that > max length\n",
    "        max_length=context_length,\n",
    "        return_attention_mask=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    input_ids = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "    \n",
    "    # find the last assistant response\n",
    "    last_assistant_idx = max(\n",
    "        idx for idx, turn in enumerate(conversation)\n",
    "        if turn[\"role\"] == ASSISTANT_ROLE_NAME\n",
    "    )\n",
    "\n",
    "    # Use <im_start> and <im_end> tokens instead of BOS/EOS\n",
    "    im_start_token_id = 52000  # <im_start>\n",
    "    im_end_token_id = 52001    # <im_end>\n",
    "    \n",
    "    current_token_idx = 0\n",
    "    for turn_idx, turn in enumerate(conversation):\n",
    "        role = turn[\"role\"]\n",
    "        content = turn[\"content\"]\n",
    "\n",
    "        try:\n",
    "            start_of_turn_bos_idx = input_ids.index(im_start_token_id, current_token_idx)\n",
    "        except ValueError:\n",
    "            break \n",
    "\n",
    "        search_for_eos_from = start_of_turn_bos_idx + 1 # search after the current <im_start>\n",
    "        end_of_turn_eos_idx = -1\n",
    "\n",
    "        for k_eos in range(search_for_eos_from, len(input_ids)):\n",
    "            if input_ids[k_eos] == im_end_token_id:\n",
    "                end_of_turn_eos_idx = k_eos\n",
    "                break\n",
    "        if end_of_turn_eos_idx == -1:\n",
    "            print(f\"Warning: Could not find <im_end> token for turn: {turn}\")\n",
    "            return None\n",
    "\n",
    "        role_and_newline_text = f\"{role}\\n\"\n",
    "        role_and_newline_tokens = tokenizer.encode(role_and_newline_text, add_special_tokens=False)\n",
    "\n",
    "        # The actual start of content tokens\n",
    "        start_of_content_idx = start_of_turn_bos_idx + 1 + len(role_and_newline_tokens) # +1 for <im_start>\n",
    "\n",
    "        if role == ASSISTANT_ROLE_NAME and turn_idx == last_assistant_idx:\n",
    "            # unmask tokens from start_of_content_idx up to (but not including) end_of_turn_eos_idx\n",
    "            for k_label in range(start_of_content_idx, end_of_turn_eos_idx + 1):\n",
    "                if k_label >= 0 and k_label < len(labels):\n",
    "                    labels[k_label] = input_ids[k_label]\n",
    "        \n",
    "        current_token_idx = end_of_turn_eos_idx + 1\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized = {}\n",
    "for split in [\"train\", \"valid\"]:\n",
    "    tokenized[split] = (\n",
    "        Dataset.from_dict(filtered_convos[split])\n",
    "        .map(formatting_func)\n",
    "        .select_columns([\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    )\n",
    "    tokenized[split].save_to_disk(f\"{DATA_DIR}/{dataset_name}/tokenized_{context_length}_{split}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
