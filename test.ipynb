{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "n_samples = 2_000_000\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be830d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f699ff02f74e45939a9c6b15e8159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = \"/nobackup1/wyf/\"\n",
    "\n",
    "# Takes like 30s to load (it's bad)\n",
    "raw_dataset = load_dataset(\n",
    "    \"mlfoundations/dclm-baseline-1.0\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating split datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 35225/2000000 [00:29<26:31, 1234.42it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def filter_dataset(dataset, n_samples: int = None):\n",
    "    # filtered = []\n",
    "    # for sample in tqdm(iter(dataset[\"train\"].take(n_samples)), total=n_samples):\n",
    "    #     # IMPORTANT REVERSAL STEP\n",
    "    #     filtered.append(sample[\"text\"][::-1])\n",
    "    # return filtered\n",
    "\n",
    "    return (\n",
    "        dataset\n",
    "            .select_columns([\"text\"])\n",
    "            .map(lambda s: {\"text\": s[\"text\"][::-1]})\n",
    "    )\n",
    "    \n",
    "# 1k examples: 4.0s\n",
    "\n",
    "print(\"Generating split datasets...\")\n",
    "raw_dataset_with_tqdm = [x for x in tqdm(raw_dataset.take(n_samples), total=n_samples)]\n",
    "split_datasets = (\n",
    "    Dataset.from_list(list(raw_dataset_with_tqdm))\n",
    "        .train_test_split(test_size=0.1, seed=0)\n",
    ")\n",
    "datasets = DatasetDict({\n",
    "    \"train\": filter_dataset(split_datasets[\"train\"]),\n",
    "    \"valid\": filter_dataset(split_datasets[\"test\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, dataset in datasets.items():\n",
    "    dataset.to_parquet(f\"./data/dclm_{n_samples}/{split_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e8e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_parquet(f\"./data/dclm_{n_samples}/train.parquet\"),\n",
    "    \"valid\": Dataset.from_parquet(f\"./data/dclm_{n_samples}/valid.parquet\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4s on 1k examples\n",
    "# 3m 30s on 200k examples\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_iterator():\n",
    "    for x in tqdm(datasets[\"train\"][\"text\"]):\n",
    "        yield x\n",
    "\n",
    "spm_tokenizer = SentencePieceBPETokenizer()\n",
    "spm_tokenizer.train_from_iterator(\n",
    "    text_iterator(),\n",
    "    vocab_size=52_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = tokenizer.train_new_from_iterator(text_iterator(), 52_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ef6cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[32m      3\u001b[39m tokenizer = PreTrainedTokenizerFast(\n\u001b[32m      4\u001b[39m     tokenizer_object=spm_tokenizer,\n\u001b[32m      5\u001b[39m     bos_token=\u001b[33m\"\u001b[39m\u001b[33m<s>\u001b[39m\u001b[33m\"\u001b[39m,           \u001b[38;5;66;03m# Always added at start\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     pad_token=\u001b[33m\"\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m\"\u001b[39m,         \u001b[38;5;66;03m# Used for padding shorter sequences\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./tokenizers/spm_200k\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/__init__.py:982\u001b[39m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    980\u001b[39m _import_structure = {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure.items()}\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m import_structure = \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})].update(_import_structure)\n\u001b[32m    985\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\n\u001b[32m    986\u001b[39m     \u001b[34m__name__\u001b[39m,\n\u001b[32m    987\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m     extra_objects={\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m: __version__},\n\u001b[32m    991\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/import_utils.py:2825\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2801\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2803\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2804\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2805\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2823\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2824\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2825\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2826\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/import_utils.py:2538\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2536\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2537\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2538\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2541\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/import_utils.py:2562\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2560\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2562\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=spm_tokenizer,\n",
    "    bos_token=\"<s>\",           # Always added at start\n",
    "    eos_token=\"</s>\",          # Always added at end  \n",
    "    unk_token=\"<unk>\",         # Replaces unknown words\n",
    "    pad_token=\"<pad>\",         # Used for padding shorter sequences\n",
    ")\n",
    "tokenizer.save_pretrained(\"./tokenizers/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8327d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./tokenizers/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES CONTEXT LENGTH\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048e5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93827b391d5d4d4797630b95f288c063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314ef90d6bf84aa9937687c3eaccfe30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 25s to parse 1k examples\n",
    "# 4m 40s to parse 10k examples\n",
    "# 7m 50s to parse 200k examples\n",
    "tokenized_dataset = datasets[\"train\"].map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "tokenized_dataset_valid = datasets[\"valid\"].map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970c94bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4867fd8c1f7c49d4b3ebd18bf8a3fb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/128 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44998ac6d40843588c9f533cd49948cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60716900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0s to save (wow)\n",
    "tokenized_dataset.to_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid.to_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460233be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = Dataset.from_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid = Dataset.from_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07ed284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 127541\n",
      "})\n",
      "Produced dataset of 127,541 rows, 1024 tokens each\n",
      "Total tokens: 130,601,984\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)\n",
    "print(f\"Produced dataset of {tokenized_dataset.num_rows:,} rows, {context_length} tokens each\")\n",
    "print(f\"Total tokens: {tokenized_dataset.num_rows * context_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbbcf7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 52003\n",
      "Model config vocab size: 52000\n",
      "BOS token ID: 52000\n",
      "EOS token ID: 52001\n",
      "PAD token ID: 52002\n",
      "Sample tokens: {'input_ids': [590, 565, 569, 5964, 33513], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model config vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Check a sample tokenization\n",
    "sample_text = \"hello world\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(f\"Sample tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b018d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model on meta device\n"
     ]
    }
   ],
   "source": [
    "# 3.2s to initialize model\n",
    "\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_size = \"2B\"\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    hidden_size=2048 if model_size == \"2B\" else 3072,\n",
    "    intermediate_size=16384 if model_size == \"2B\" else 24576,\n",
    "    num_hidden_layers=18 if model_size == \"2B\" else 28,\n",
    "    num_attention_heads=8 if model_size == \"2B\" else 16,\n",
    "    num_key_value_heads=1 if model_size == \"2B\" else 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_scaling=None,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model = LlamaForCausalLM(config)\n",
    "    print(f\"Initialized model on meta device\")\n",
    "\n",
    "model = model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85def139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 2194.9M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c0fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2194487/1457779597.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 0.1s to initialize training args\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"reverse-model-2B\",\n",
    "    \n",
    "    # Batch size settings - LEDOM uses global batch size of 1024 sequences\n",
    "    per_device_train_batch_size=1,  # Micro-batch size per GPU\n",
    "    per_device_eval_batch_size=1,   # Used in their fine-tuning setup\n",
    "    gradient_accumulation_steps=1, # To achieve global batch size (adjust based on GPU count)\n",
    "\n",
    "    eval_strategy=\"steps\",        # Evaluate every N steps\n",
    "    eval_steps=5000,     # Eval every N steps  \n",
    "    logging_steps=1,  # More frequent logging to match their monitoring\n",
    "    \n",
    "    # Training duration - LEDOM trained for ~51,900 iterations for 7B model\n",
    "    num_train_epochs=1,  # Keep as 1 epoch since they trained on 435B tokens once\n",
    "    \n",
    "    # Optimizer settings - match LEDOM exactly\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,           # Peak learning rate: 2×10⁻⁴ \n",
    "    weight_decay=0.1,             # Matches their setting\n",
    "    adam_beta1=0.9,               # Adam β₁\n",
    "    adam_beta2=0.95,              # Adam β₂  \n",
    "    adam_epsilon=1e-8,            # Adam ε\n",
    "    \n",
    "    # Learning rate schedule - LEDOM uses cosine with specific warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,            # LEDOM uses 2000 warmup iterations\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=1.0,            # Gradient clipping norm\n",
    "    \n",
    "    # Precision - LEDOM uses BF16, not FP16\n",
    "    bf16=True,                    # Use BF16 instead of FP16\n",
    "    fp16=False,                   # Disable FP16\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=3,           # Reasonable limit for storage\n",
    "    save_only_model=True,\n",
    "    \n",
    "    # Additional LEDOM-specific settings\n",
    "    dataloader_num_workers=2,     # For efficiency\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    \n",
    "    # Disable features not used in LEDOM training\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96def02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda0bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='505' max='63771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  505/63771 03:09 < 6:36:42, 2.66 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1m for 1k samples (2.2M tokens)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/trainer.py:2237\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2235\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/trainer.py:2578\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2571\u001b[39m context = (\n\u001b[32m   2572\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2574\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2575\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2576\u001b[39m )\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2581\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2582\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2583\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2584\u001b[39m ):\n\u001b[32m   2585\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2586\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/trainer.py:3840\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3838\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3840\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/accelerate/accelerator.py:2578\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1m for 1k samples (2.2M tokens)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5a28",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from: ./reverse-model/checkpoint-191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", model=\"./reverse-model/checkpoint-9\", device=device, \n",
    "# )\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Base model directory\n",
    "base_dir = \"./reverse-model\"\n",
    "\n",
    "# Find the first subdirectory (sorted for consistency)\n",
    "subdirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "if not subdirs:\n",
    "    raise FileNotFoundError(f\"No subdirectories found in {base_dir}\")\n",
    "first_checkpoint = os.path.join(base_dir, sorted(subdirs)[0])\n",
    "\n",
    "print(f\"Using model from: {first_checkpoint}\")\n",
    "\n",
    "# Device selection\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=first_checkpoint,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32809ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlrow olleh reht gnet eht esa ta ereht dnet,medla eht ht,ni IP,ekaL t rotfollaec ednaL nadnotseht a ta eniht e t yti \" hw tita ni ehgnica s.t tih ro ej dehtnia yapivah t ehtuot decuop evaca ehta sit esira dehtca seroit t seva t suo eesw ra eh et e st deht'uo na d \n",
      " d an ou'thed ts e te he ar wsee ous t aves t tiores acthed arise tis athe acave pouced touthe t havipay ainthed je or hit t.s acinghe in atit wh \" ity t e thine at a thestondan Lande cealloftor t Lake,PI in,th the aldem,tend there at ase the teng ther hello world\n",
      "['world▁', 'hello▁', 'ther▁', 'g▁', 'ten', 'the▁', 'ase▁', 'at▁', 'there▁', 'tend▁', ',', 'dem', 'al', 'the▁', 'th▁', ',', 'in', 'PI▁', ',', 'ake', 'L', 't▁', 'tor▁', 'of', 'all', 'ce', 'e▁', 'Land', 'an▁', 'ond', 'st', 'the', 'a▁', 'at▁', 'thine▁', 'e▁', 't▁', 'ity▁', '\"▁', '▁', 'wh', 't▁', 'ati', 'in▁', 'he▁', 'ing', 'ac', 's▁', 't.', 'hit▁', 'or▁', 'je▁', 'thed▁', 'ain', 'pay▁', 'vi', 'ha', 't▁', 'the▁', 'tou', 'ed▁', 'ouc', 'p', 'ave▁', 'ac', 'athe▁', 'tis▁', 'arise▁', 'thed▁', 'ac', 'ores▁', 'ti', 't▁', 'aves▁', 't▁', 'ous▁', 'see▁', 'w', 'ar▁', 'he▁', 'te▁', 'e▁', 'ts▁', 'thed▁', \"'\", 'ou', 'an▁', 'd▁', '▁']\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\" hello world\"[::-1], num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "print(text)\n",
    "print(text[::-1])\n",
    "\n",
    "print([token[::-1] for token in tokenizer.tokenize(text)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
