{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "n_samples = 10_000_000_000\n",
    "context_length = 4096\n",
    "\n",
    "# DATA_DIR = \"/home/wyf/ai/causal-llm/data\"\n",
    "DATA_DIR = \"/home/wyf/orcd/pool/causal-llm/data\"\n",
    "TOKENIZER_DIR = \"/home/wyf/ai/causal-llm/tokenizers\"\n",
    "MODEL_DIR = \"/home/wyf/ai/causal-llm/models\"\n",
    "\n",
    "dataset = \"fineweb-10BT\"\n",
    "model_name = f\"reverse-{dataset}-ctx-{context_length}-2B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be830d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f5a90807e44bec9da2b171604e4d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454093868d9e4fa2b53fc62b448383dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2167201a607f4587a39db9c1afda960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Takes like 30s to load (it's bad)\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-edu\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/home/wyf/orcd/pool/hf-datasets/\",\n",
    "    name=\"sample-10BT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def filter_dataset(dataset, n_samples: int = None):\n",
    "    return (\n",
    "        dataset\n",
    "            .select_columns([\"text\"])\n",
    "            .map(lambda s: {\"text\": s[\"text\"][::-1]})\n",
    "    )\n",
    "    \n",
    "# 1k examples: 4.0s\n",
    "\n",
    "print(\"Generating split datasets...\")\n",
    "raw_dataset_with_tqdm = [x for x in tqdm(raw_dataset.take(n_samples), total=n_samples)]\n",
    "split_datasets = (\n",
    "    Dataset.from_list(list(raw_dataset_with_tqdm))\n",
    "        .train_test_split(test_size=0.005, seed=0)\n",
    ")\n",
    "datasets = DatasetDict({\n",
    "    \"train\": filter_dataset(split_datasets[\"train\"]),\n",
    "    \"valid\": filter_dataset(split_datasets[\"test\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, dataset in datasets.items():\n",
    "    dataset.to_parquet(f\"{DATA_DIR}/fineweb_{n_samples}/{split_name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54292436",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e8e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae388b9be5b44d72b28656ad96333660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "# Dataset is too big to fit into memory so we stream\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": Dataset.load_from_disk(f\"{DATA_DIR}/{dataset}/train\"),\n",
    "    \"valid\": Dataset.load_from_disk(f\"{DATA_DIR}/{dataset}/val\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e72b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pihsnoitaler lanosrep ,efil ,tnemevom si ereht gnieb lanrete s'doG nI -\n",
      ".pihsnoitaler ni efil lanosrep si doG fo efil lanrete eht taht mriffa ot si enuirt si doG taht ssefnoc oT .1\n",
      ":)28-67 segap morf setouq selipmoc swollof tahw( stnemetats eerht htiw rammarg siht sezirammus eH .)67p( \"efil derahs dna ,ytilautum ,ytinummoc setaerc dna srehto ot flesti fo sevig yleerf taht evol enivid suordnow fo rammarg\" a su gnivig sa htiaf nairatinirt fo skaeps eroilgiM\n",
      "evol enivid fo rammarg eht :ytinirT ehT\n"
     ]
    }
   ],
   "source": [
    "print(list(split_datasets[\"train\"].take(1))[0][\"text\"][500::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer (7.4s on 1k examples)\n",
    "# 3m 30s on 200k examples\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: take sample\n",
    "def text_iterator():\n",
    "    for x in tqdm(datasets[\"train\"][\"text\"]):\n",
    "        yield x\n",
    "\n",
    "spm_tokenizer = SentencePieceBPETokenizer()\n",
    "spm_tokenizer.train_from_iterator(\n",
    "    text_iterator(),\n",
    "    vocab_size=52_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=spm_tokenizer,\n",
    "    bos_token=\"<s>\",           # Always added at start\n",
    "    eos_token=\"</s>\",          # Always added at end  \n",
    "    unk_token=\"<unk>\",         # Replaces unknown words\n",
    "    pad_token=\"<pad>\",         # Used for padding shorter sequences\n",
    ")\n",
    "tokenizer.save_pretrained(\"./tokenizers/fineweb_spm_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8327d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_spm_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbcf8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 4096\n"
     ]
    }
   ],
   "source": [
    "# USES CONTEXT LENGTH\n",
    "\n",
    "context_length = 4096\n",
    "print(f\"Context length: {context_length}\")\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7098f55a4a34ad9a77232737f1f8181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 25s to parse 1k examples\n",
    "# 4m 40s to parse 10k examples\n",
    "# 7m 50s to parse 200k examples\n",
    "\n",
    "tokenized_dataset_train = split_datasets[\"train\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "tokenized_dataset_valid = split_datasets[\"valid\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "970c94bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Takes a hot minute to save b/c streaming\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 30min for 2M fineweb examples\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 3m for 1/9 of that\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving training dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtokenized_dataset_train\u001b[49m.to_parquet(\n\u001b[32m      7\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_train.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving valid dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m tokenized_dataset_valid.to_parquet(\n\u001b[32m     10\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_valid.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Takes a hot minute to save b/c streaming\n",
    "# 30min for 2M fineweb examples\n",
    "# 3m for 1/9 of that\n",
    "\n",
    "print(\"Saving training dataset...\")\n",
    "tokenized_dataset_train.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}/tokenized_{context_length}_train.parquet\")\n",
    "print(\"Saving valid dataset...\")\n",
    "tokenized_dataset_valid.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}/tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460233be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d38675995a4fe0bc518eb4ec1dd6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37d69b8ce1a4fd6bf8b981b4515e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da3c1e389fe44a99d32d37bbdc41a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenized datasets\n",
    "tokenized_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        # 1.8.4.M documents, ~30k/s\n",
    "        \"train\": f\"{DATA_DIR}/{dataset}/tokenized_{context_length}_train.parquet\",\n",
    "        # ~870 documents\n",
    "        \"valid\": f\"{DATA_DIR}/{dataset}/tokenized_{context_length}_valid.parquet\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73831fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_4096_train.parquet  tokenized_8192_valid.parquet  val\n",
      "tokenized_4096_valid.parquet  train\t\t\t    val.parquet\n",
      "tokenized_8192_train.parquet  train.parquet\n",
      "  File: /home/wyf/orcd/pool/causal-llm/data/fineweb-10BT/tokenized_4096_train.parquet\n",
      "  Size: 11611680457\tBlocks: 22673750   IO Block: 1048576 regular file\n",
      "Device: 1000e2h/1048802d\tInode: 8451        Links: 1\n",
      "Access: (0664/-rw-rw-r--)  Uid: (258746/     wyf)   Gid: (258746/     wyf)\n",
      "Access: 2025-08-16 10:44:36.235163668 -0400\n",
      "Modify: 2025-08-16 04:39:40.625031979 -0400\n",
      "Change: 2025-08-16 04:39:40.625031979 -0400\n",
      " Birth: -\n"
     ]
    }
   ],
   "source": [
    "! ls ~/orcd/pool/causal-llm/data/fineweb-10BT\n",
    "! stat ~/orcd/pool/causal-llm/data/fineweb-10BT/tokenized_4096_train.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ebb655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dymal Cell Progeny, Extracellular Matrix Molecules, and Axons in Spinal Cord Scar Tissue\n",
      "The scar tissue that forms at spinal cord injuries is thought to inhibit axonal growth [33,34]. Chondroitin sulphate proteoglycans (CSPG) appear to be the principal axonal growth inhibiting molecules in glial scars . Ependyma-derived cells at the injury formed a complementary nonoverlapping pattern with areas that were CSPG immunoreactive (Figure 7A and 7B), indicating that ependymal cell progeny do not cont\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(tokenizer.decode(tokenized_dataset[\"train\"][5][\"input_ids\"]))[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07ed284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 6\n",
      "})\n",
      "Produced dataset of 6 rows, 4906 tokens each\n",
      "Total tokens: 29,436\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)\n",
    "print(f\"Produced dataset of {tokenized_dataset.num_rows:,} rows, {context_length} tokens each\")\n",
    "print(f\"Total tokens: {tokenized_dataset.num_rows * context_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbbcf7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 52003\n",
      "Model config vocab size: 52000\n",
      "BOS token ID: 52000\n",
      "EOS token ID: 52001\n",
      "PAD token ID: 52002\n",
      "Sample tokens: {'input_ids': [1078, 1143, 1055, 2648, 77, 69], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model config vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Check a sample tokenization\n",
    "sample_text = \"hello world\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(f\"Sample tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b018d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model on meta device\n"
     ]
    }
   ],
   "source": [
    "# 3.2s to initialize model\n",
    "\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_size = \"2B\"\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    hidden_size=2048 if model_size == \"2B\" else 3072,\n",
    "    intermediate_size=16384 if model_size == \"2B\" else 24576,\n",
    "    num_hidden_layers=18 if model_size == \"2B\" else 28,\n",
    "    num_attention_heads=8 if model_size == \"2B\" else 16,\n",
    "    num_key_value_heads=1 if model_size == \"2B\" else 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_scaling=None,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model = LlamaForCausalLM(config)\n",
    "    print(f\"Initialized model on meta device\")\n",
    "\n",
    "model = model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85def139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 2194.9M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a03b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c0fd4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[32m      5\u001b[39m args = TrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=model_name,\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     report_to=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m trainer = Trainer(\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     model=\u001b[43mmodel\u001b[49m,\n\u001b[32m     55\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     56\u001b[39m     args=args,\n\u001b[32m     57\u001b[39m     data_collator=data_collator,\n\u001b[32m     58\u001b[39m     train_dataset=tokenized_dataset_train,\n\u001b[32m     59\u001b[39m     eval_dataset=tokenized_dataset_valid,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized_dataset_train[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 0.1s to initialize training args\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    \n",
    "    # Batch size settings - LEDOM uses global batch size of 1024 sequences\n",
    "    per_device_train_batch_size=2,  # Micro-batch size per GPU\n",
    "    per_device_eval_batch_size=1,   # Used in their fine-tuning setup\n",
    "    gradient_accumulation_steps=1, # To achieve global batch size (adjust based on GPU count)\n",
    "\n",
    "    eval_strategy=\"steps\",        # Evaluate every N steps\n",
    "    eval_steps=5000,     # Eval every N steps  \n",
    "    logging_steps=1,  # More frequent logging to match their monitoring\n",
    "    \n",
    "    # Training duration - LEDOM trained for ~51,900 iterations for 7B model\n",
    "    num_train_epochs=1,  # Keep as 1 epoch since they trained on 435B tokens once\n",
    "    \n",
    "    # Optimizer settings - match LEDOM exactly\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,           # Peak learning rate: 2×10⁻⁴ \n",
    "    weight_decay=0.1,             # Matches their setting\n",
    "    adam_beta1=0.9,               # Adam β₁\n",
    "    adam_beta2=0.95,              # Adam β₂  \n",
    "    adam_epsilon=1e-8,            # Adam ε\n",
    "    \n",
    "    # Learning rate schedule - LEDOM uses cosine with specific warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,            # LEDOM uses 2000 warmup iterations\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=1.0,            # Gradient clipping norm\n",
    "    \n",
    "    # Precision - LEDOM uses BF16, not FP16\n",
    "    bf16=True,                    # Use BF16 instead of FP16\n",
    "    fp16=False,                   # Disable FP16\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=3,           # Reasonable limit for storage\n",
    "    save_only_model=True,\n",
    "    \n",
    "    # Additional LEDOM-specific settings\n",
    "    dataloader_num_workers=2,     # For efficiency\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    \n",
    "    # Disable features not used in LEDOM training\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    ")\n",
    "\n",
    "print(len(tokenized_dataset_train[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96def02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m for 1k samples (2.2M tokens)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5a28",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Device selection\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d74385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_spm_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7c2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁dlrow', '▁olle', 'h']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hello world\"[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf4031d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdefacf275744916a9bb0dfba27b290b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=f\"{MODEL_DIR}/reverse-model-2B-fineweb-2000000-batchsize-20/checkpoint-22109\",\n",
    "    model=f\"{MODEL_DIR}/reverse-model-2B-fineweb-10BT-ctx-4096-batchsize-5/checkpoint-6000\",\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    top_p=1.0,\n",
    "    temperature=1.0,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722fef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era snrocinU\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "Unicorns are\n",
    "\"\"\"[::-1].strip()\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66814e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEGIN GENERATED TEXT [REVERSED] ===\n",
      "eht yb drawrof devom eerga dluow elpoep eseht lla dna ssenevitceffe latnemtraped dnoces a eb neht dluow ereht trats tsrif rieht morf dna ,ylerutamerp yawrednu teg ot saw noitnetni ’srekcatta eht yllacipyT\n",
      ".htgnerts lluf sti rof gnithgif eht fo esruoc eht ni tnaveler gnihton ees ot dellirht saw bom ehT .elim erauqs hcae ni detautis yllanosrep stnatibahni eht htiw citehtapmys cilbup rieht gnivah yb mrofinu snwot niatrec ekam depleh taht ssenevitceffe latnemtraped a htiw desopxe ngierof tsrif eht ni detacidni taerht reehs eht dna ,redrob ni noitcirtsUnicorns are\n",
      "\n",
      "era snrocinUstriction in border, and the sheer threat indicated in the first foreign exposed with a departmental effectiveness that helped make certain towns uniform by having their public sympathetic with the inhabitants personally situated in each square mile. The mob was thrilled to see nothing relevant in the course of the fighting for its full strength.\n",
      "Typically the attackers’ intention was to get underway prematurely, and from their first start there would then be a second departmental effectiveness and all these people would agree moved forward by the\n"
     ]
    }
   ],
   "source": [
    "text = pipe(test_text, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "print(f\"=== BEGIN GENERATED TEXT [REVERSED] ===\")\n",
    "print(text[::-1])\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d733d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f845e4e54f8d41d1a81d37d2adbc6ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test direct token generation\n",
    "import torch\n",
    "\n",
    "# Load model directly (not pipeline)\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # f\"{MODEL_DIR}/reverse-model-2B-fineweb-2000000-batchsize-20/checkpoint-22109\",\n",
    "    f\"{MODEL_DIR}/reverse-model-2B-fineweb-10BT-ctx-4096-batchsize-5/checkpoint-3000\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7942fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 aownftime tissuPublschedule end Ophthalmsysthpractices ets. on, quito tal, .)is the first planet in the solar system.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated[::-1]  # Reverse back to normal\n",
    "\n",
    "# Test it\n",
    "result = generate_text(\"is the first planet in the solar system.\"[::-1], max_tokens=None)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
