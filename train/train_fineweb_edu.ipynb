{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "n_samples = 2_000_000\n",
    "context_length = 1024\n",
    "\n",
    "# DATA_DIR = \"/home/wyf/ai/causal-llm/data\"\n",
    "DATA_DIR = \"/home/wyf/orcd/pool/causal-llm/data\"\n",
    "TOKENIZER_DIR = \"/home/wyf/ai/causal-llm/tokenizers\"\n",
    "MODEL_DIR = \"/home/wyf/ai/causal-llm/models\"\n",
    "\n",
    "dataset = \"fineweb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be830d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a01eafa46764e4eb75ad8b7b253ce18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dfc55b656641caab058981bb12ceb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "000_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa0324fe9554b3c85117447941e96a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "001_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0448750f85e430987138885e0b29ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "002_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1fc155b7e1479892397aa4f2ccb08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "003_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3961f15f10354c65a88d727955b34852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "004_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d6497d907e4b9ca34036bc1c7dbc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "005_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970511ef6d2740b1a8ed8e970b7cd9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "006_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f808b8c9b104800ae6bc7413897c17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "007_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de216a851bb4db5b3c2c2a6eecd3592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "008_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84722453f1fb4459928fbd95b09c0db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "009_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6485a45a4644b5db0da457f9fecadc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "010_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60fbabc89504d42bcb58475d424ac96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "011_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd19781bf25471ab9a4e6e29040d4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "012_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe27b00c75434d4fb8683c5520e3bab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "013_00000.parquet:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0516c2c570c4b0da5a2f2c51777cf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9672101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90545b143da42ccad770f0ffc895bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Takes like 30s to load (it's bad)\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-edu\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"~/orcd/pool/hf-datasets/\",\n",
    "    name=\"sample-10BT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c4acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network With Us\n",
      "Join us on Facebook to get the latest news and updates.\n",
      "Lauren Boulden's Story\n",
      "Using Think-Alouds to Get Inside Langston Hughes' Head\n",
      "Over my past few years of teaching, there have bee\n",
      "['▁eeb', '▁evah', '▁ereht', '▁,gnihcaet', '▁fo', '▁sraey', '▁wef', '▁tsap', '▁ym', '▁revO', '\\n', 'daeH', \"▁'se\", 'hguH', '▁notsgnaL', '▁edisnI', '▁teG', '▁ot', '▁sd', 'uo', 'lA', '-', 'knihT', '▁gnisU', '\\ny', 'rotS', \"▁s'ne\", 'dluoB', '▁ne', 'ruaL', '\\n.se', 'tadpu', '▁dna', '▁swen', '▁tsetal', '▁eht', '▁teg', '▁ot', '▁koobecaF', '▁no', '▁su', '▁nioJ', '\\ns', 'U', '▁htiW', '▁krowteN']\n"
     ]
    }
   ],
   "source": [
    "text = raw_dataset.take(100)[10][\"text\"][:200]\n",
    "print(text)\n",
    "print(tokenizer.tokenize(text[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def filter_dataset(dataset, n_samples: int = None):\n",
    "    return (\n",
    "        dataset\n",
    "            .select_columns([\"text\"])\n",
    "            .map(lambda s: {\"text\": s[\"text\"][::-1]})\n",
    "    )\n",
    "    \n",
    "# 1k examples: 4.0s\n",
    "\n",
    "print(\"Generating split datasets...\")\n",
    "raw_dataset_with_tqdm = [x for x in tqdm(raw_dataset.take(n_samples), total=n_samples)]\n",
    "split_datasets = (\n",
    "    Dataset.from_list(list(raw_dataset_with_tqdm))\n",
    "        .train_test_split(test_size=0.005, seed=0)\n",
    ")\n",
    "datasets = DatasetDict({\n",
    "    \"train\": filter_dataset(split_datasets[\"train\"]),\n",
    "    \"valid\": filter_dataset(split_datasets[\"test\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, dataset in datasets.items():\n",
    "    dataset.to_parquet(f\"{DATA_DIR}/fineweb_{n_samples}/{split_name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54292436",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset is too big to fit into memory so we stream\n",
    "datasets = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": f\"{DATA_DIR}/fineweb_{n_samples}/train.parquet\",\n",
    "        \"valid\": f\"{DATA_DIR}/fineweb_{n_samples}/valid.parquet\",\n",
    "    },\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e72b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(datasets[\"train\"].take(1))[0][\"text\"][500::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer (7.4s on 1k examples)\n",
    "# 3m 30s on 200k examples\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_iterator():\n",
    "    for x in tqdm(datasets[\"train\"][\"text\"]):\n",
    "        yield x\n",
    "\n",
    "spm_tokenizer = SentencePieceBPETokenizer()\n",
    "spm_tokenizer.train_from_iterator(\n",
    "    text_iterator(),\n",
    "    vocab_size=52_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=spm_tokenizer,\n",
    "    bos_token=\"<s>\",           # Always added at start\n",
    "    eos_token=\"</s>\",          # Always added at end  \n",
    "    unk_token=\"<unk>\",         # Replaces unknown words\n",
    "    pad_token=\"<pad>\",         # Used for padding shorter sequences\n",
    ")\n",
    "tokenizer.save_pretrained(\"./tokenizers/fineweb_spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8327d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES CONTEXT LENGTH\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25s to parse 1k examples\n",
    "# 4m 40s to parse 10k examples\n",
    "# 7m 50s to parse 200k examples\n",
    "\n",
    "tokenized_dataset = datasets[\"train\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "tokenized_dataset_valid = datasets[\"valid\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a hot minute to save b/c streaming\n",
    "# 30min for 2M fineweb examples\n",
    "# 3m for 1/9 of that\n",
    "\n",
    "print(\"Saving training dataset...\")\n",
    "tokenized_dataset.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}.parquet\")\n",
    "print(\"Saving valid dataset...\")\n",
    "tokenized_dataset_valid.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460233be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized datasets\n",
    "tokenized_dataset = Dataset.from_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid = Dataset.from_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ed284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset)\n",
    "print(f\"Produced dataset of {tokenized_dataset.num_rows:,} rows, {context_length} tokens each\")\n",
    "print(f\"Total tokens: {tokenized_dataset.num_rows * context_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcf7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model config vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Check a sample tokenization\n",
    "sample_text = \"hello world\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(f\"Sample tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2s to initialize model\n",
    "\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_size = \"2B\"\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    hidden_size=2048 if model_size == \"2B\" else 3072,\n",
    "    intermediate_size=16384 if model_size == \"2B\" else 24576,\n",
    "    num_hidden_layers=18 if model_size == \"2B\" else 28,\n",
    "    num_attention_heads=8 if model_size == \"2B\" else 16,\n",
    "    num_key_value_heads=1 if model_size == \"2B\" else 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_scaling=None,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model = LlamaForCausalLM(config)\n",
    "    print(f\"Initialized model on meta device\")\n",
    "\n",
    "model = model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85def139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1s to initialize training args\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"reverse-model-2B\",\n",
    "    \n",
    "    # Batch size settings - LEDOM uses global batch size of 1024 sequences\n",
    "    per_device_train_batch_size=1,  # Micro-batch size per GPU\n",
    "    per_device_eval_batch_size=1,   # Used in their fine-tuning setup\n",
    "    gradient_accumulation_steps=1, # To achieve global batch size (adjust based on GPU count)\n",
    "\n",
    "    eval_strategy=\"steps\",        # Evaluate every N steps\n",
    "    eval_steps=5000,     # Eval every N steps  \n",
    "    logging_steps=1,  # More frequent logging to match their monitoring\n",
    "    \n",
    "    # Training duration - LEDOM trained for ~51,900 iterations for 7B model\n",
    "    num_train_epochs=1,  # Keep as 1 epoch since they trained on 435B tokens once\n",
    "    \n",
    "    # Optimizer settings - match LEDOM exactly\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,           # Peak learning rate: 2×10⁻⁴ \n",
    "    weight_decay=0.1,             # Matches their setting\n",
    "    adam_beta1=0.9,               # Adam β₁\n",
    "    adam_beta2=0.95,              # Adam β₂  \n",
    "    adam_epsilon=1e-8,            # Adam ε\n",
    "    \n",
    "    # Learning rate schedule - LEDOM uses cosine with specific warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,            # LEDOM uses 2000 warmup iterations\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=1.0,            # Gradient clipping norm\n",
    "    \n",
    "    # Precision - LEDOM uses BF16, not FP16\n",
    "    bf16=True,                    # Use BF16 instead of FP16\n",
    "    fp16=False,                   # Disable FP16\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=3,           # Reasonable limit for storage\n",
    "    save_only_model=True,\n",
    "    \n",
    "    # Additional LEDOM-specific settings\n",
    "    dataloader_num_workers=2,     # For efficiency\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    \n",
    "    # Disable features not used in LEDOM training\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m for 1k samples (2.2M tokens)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5a28",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8528e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Device selection\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d74385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baf4031d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad0eb63b3fa4d7d829887149cfbb374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=f\"{MODEL_DIR}/reverse-model-2B-fineweb-2000000-batchsize-20/checkpoint-22109\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    top_p=1.0,\n",
    "    temperature=1.0,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "722fef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    ", as the fifth planet in our solar system.\n",
    "\"\"\"[::-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66814e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pipe(test_text, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "print(f\"=== BEGIN GENERATED TEXT [REVERSED] ===\")\n",
    "print(text[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d733d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8081176c2b1d412fbe2fe416a4da4a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test direct token generation\n",
    "import torch\n",
    "\n",
    "# Load model directly (not pipeline)\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"{MODEL_DIR}/reverse-model-2B-fineweb-2000000-batchsize-20/checkpoint-22109\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7942fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planets than any other body in our solar system. So it is interesting to look at this planet because it is the first planet in the solar system.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated[::-1]  # Reverse back to normal\n",
    "\n",
    "# Test it\n",
    "result = generate_text(\"is the first planet in the solar system.\"[::-1], max_tokens=None)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
