{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "n_samples = 2_000_000\n",
    "context_length = 1024\n",
    "\n",
    "# DATA_DIR = \"/home/wyf/ai/causal-llm/data\"\n",
    "DATA_DIR = \"/home/wyf/orcd/pool/causal-llm/data\"\n",
    "TOKENIZER_DIR = \"/home/wyf/ai/causal-llm/tokenizers\"\n",
    "MODEL_DIR = \"/home/wyf/ai/causal-llm/models\"\n",
    "\n",
    "dataset = \"fineweb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be830d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes like 30s to load (it's bad)\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-edu\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"~/orcd/pool/hf-datasets/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def filter_dataset(dataset, n_samples: int = None):\n",
    "    # filtered = []\n",
    "    # for sample in tqdm(iter(dataset[\"train\"].take(n_samples)), total=n_samples):\n",
    "    #     # IMPORTANT REVERSAL STEP\n",
    "    #     filtered.append(sample[\"text\"][::-1])\n",
    "    # return filtered\n",
    "\n",
    "    return (\n",
    "        dataset\n",
    "            .select_columns([\"text\"])\n",
    "            .map(lambda s: {\"text\": s[\"text\"][::-1]})\n",
    "    )\n",
    "    \n",
    "# 1k examples: 4.0s\n",
    "\n",
    "print(\"Generating split datasets...\")\n",
    "raw_dataset_with_tqdm = [x for x in tqdm(raw_dataset.take(n_samples), total=n_samples)]\n",
    "split_datasets = (\n",
    "    Dataset.from_list(list(raw_dataset_with_tqdm))\n",
    "        .train_test_split(test_size=0.1, seed=0)\n",
    ")\n",
    "datasets = DatasetDict({\n",
    "    \"train\": filter_dataset(split_datasets[\"train\"]),\n",
    "    \"valid\": filter_dataset(split_datasets[\"test\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, dataset in datasets.items():\n",
    "    dataset.to_parquet(f\"{DATA_DIR}/fineweb_{n_samples}/{split_name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54292436",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e8e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset is too big to fit into memory so we stream\n",
    "datasets = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": f\"{DATA_DIR}/fineweb_{n_samples}/train.parquet\",\n",
    "        \"valid\": f\"{DATA_DIR}/fineweb_{n_samples}/valid.parquet\",\n",
    "    },\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e72b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(datasets[\"train\"].take(1))[0][\"text\"][500::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer (7.4s on 1k examples)\n",
    "# 3m 30s on 200k examples\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_iterator():\n",
    "    for x in tqdm(datasets[\"train\"][\"text\"]):\n",
    "        yield x\n",
    "\n",
    "spm_tokenizer = SentencePieceBPETokenizer()\n",
    "spm_tokenizer.train_from_iterator(\n",
    "    text_iterator(),\n",
    "    vocab_size=52_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=spm_tokenizer,\n",
    "    bos_token=\"<s>\",           # Always added at start\n",
    "    eos_token=\"</s>\",          # Always added at end  \n",
    "    unk_token=\"<unk>\",         # Replaces unknown words\n",
    "    pad_token=\"<pad>\",         # Used for padding shorter sequences\n",
    ")\n",
    "tokenizer.save_pretrained(\"./tokenizers/fineweb_spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/fineweb_spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES CONTEXT LENGTH\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25s to parse 1k examples\n",
    "# 4m 40s to parse 10k examples\n",
    "# 7m 50s to parse 200k examples\n",
    "\n",
    "tokenized_dataset = datasets[\"train\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "tokenized_dataset_valid = datasets[\"valid\"].map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a hot minute to save b/c streaming\n",
    "# 30min for 2M fineweb examples\n",
    "# 3m for 1/9 of that\n",
    "\n",
    "print(\"Saving training dataset...\")\n",
    "tokenized_dataset.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}.parquet\")\n",
    "print(\"Saving valid dataset...\")\n",
    "tokenized_dataset_valid.to_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460233be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized datasets\n",
    "tokenized_dataset = Dataset.from_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid = Dataset.from_parquet(\n",
    "    f\"{DATA_DIR}/{dataset}_{n_samples}/tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07ed284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 884350\n",
      "})\n",
      "Produced dataset of 884,350 rows, 1024 tokens each\n",
      "Total tokens: 905,574,400\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)\n",
    "print(f\"Produced dataset of {tokenized_dataset.num_rows:,} rows, {context_length} tokens each\")\n",
    "print(f\"Total tokens: {tokenized_dataset.num_rows * context_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcf7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model config vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Check a sample tokenization\n",
    "sample_text = \"hello world\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(f\"Sample tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2s to initialize model\n",
    "\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_size = \"2B\"\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    hidden_size=2048 if model_size == \"2B\" else 3072,\n",
    "    intermediate_size=16384 if model_size == \"2B\" else 24576,\n",
    "    num_hidden_layers=18 if model_size == \"2B\" else 28,\n",
    "    num_attention_heads=8 if model_size == \"2B\" else 16,\n",
    "    num_key_value_heads=1 if model_size == \"2B\" else 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_scaling=None,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model = LlamaForCausalLM(config)\n",
    "    print(f\"Initialized model on meta device\")\n",
    "\n",
    "model = model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85def139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1s to initialize training args\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"reverse-model-2B\",\n",
    "    \n",
    "    # Batch size settings - LEDOM uses global batch size of 1024 sequences\n",
    "    per_device_train_batch_size=1,  # Micro-batch size per GPU\n",
    "    per_device_eval_batch_size=1,   # Used in their fine-tuning setup\n",
    "    gradient_accumulation_steps=1, # To achieve global batch size (adjust based on GPU count)\n",
    "\n",
    "    eval_strategy=\"steps\",        # Evaluate every N steps\n",
    "    eval_steps=5000,     # Eval every N steps  \n",
    "    logging_steps=1,  # More frequent logging to match their monitoring\n",
    "    \n",
    "    # Training duration - LEDOM trained for ~51,900 iterations for 7B model\n",
    "    num_train_epochs=1,  # Keep as 1 epoch since they trained on 435B tokens once\n",
    "    \n",
    "    # Optimizer settings - match LEDOM exactly\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,           # Peak learning rate: 2×10⁻⁴ \n",
    "    weight_decay=0.1,             # Matches their setting\n",
    "    adam_beta1=0.9,               # Adam β₁\n",
    "    adam_beta2=0.95,              # Adam β₂  \n",
    "    adam_epsilon=1e-8,            # Adam ε\n",
    "    \n",
    "    # Learning rate schedule - LEDOM uses cosine with specific warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,            # LEDOM uses 2000 warmup iterations\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=1.0,            # Gradient clipping norm\n",
    "    \n",
    "    # Precision - LEDOM uses BF16, not FP16\n",
    "    bf16=True,                    # Use BF16 instead of FP16\n",
    "    fp16=False,                   # Disable FP16\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=3,           # Reasonable limit for storage\n",
    "    save_only_model=True,\n",
    "    \n",
    "    # Additional LEDOM-specific settings\n",
    "    dataloader_num_workers=2,     # For efficiency\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    \n",
    "    # Disable features not used in LEDOM training\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m for 1k samples (2.2M tokens)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5a28",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8528e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Device selection\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d74385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{TOKENIZER_DIR}/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "859001f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁.sa', 'la', '▁,hcihw', '▁,dlrow', '▁olleh']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hello world, which, alas.\"[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baf4031d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df601395f834d2d84caf86ab320b0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=f\"{MODEL_DIR}/reverse-model-2B-fineweb-2000000-batchsize-20/checkpoint-22109\",\n",
    "    device=device,\n",
    "    top_p=1.0,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a44273c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST TEXT ===\n",
      "... for hunting, and for transportation. Ivory from their tusks is used in the manufacture of a number of items, including jewelry.\n",
      "Positive Impacts: body parts are source of valuable material; ecotourism\n",
      "\n",
      "=== TEST TEXT (truncated) ===\n",
      " parts are source of valuable material; ecotourism\n"
     ]
    }
   ],
   "source": [
    "test_text_full = list(datasets[\"train\"].take(100))[50][\"text\"]\n",
    "test_text = test_text_full[:50]\n",
    "\n",
    "print(f\"=== TEST TEXT ===\\n...{test_text_full[200::-1]}\\n\")\n",
    "print(f\"=== TEST TEXT (truncated) ===\\n{test_text[::-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "722fef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"is the first planet in the solar system.\"\"\"[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66814e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEGIN GENERATED TEXT [REVERSED] ===\n",
      "all about it,because back in the day I thought about this guy,and it sounds like everybody's still thinking about it :) ...:)\n",
      "I didn't like it very much,but I bet you told me that I shouldn't have to leave this world!That's what will happen by the end of it :) ...:)\n",
      "I pray that soon we'll be in the middle of it (oops,and we're still talking about it :) ...:)\n",
      "Oh,it's all been like that when I was a kid,so its out there,and its been for such a long time ...:)\n",
      "[the distance between Ganymede and Ganymede]\n",
      "Ganymede is the sixth planet of Jupiter and the second planet in our Solar System.It contains nine stars other than the sun.While Rhone is the second-largest and the ringmost planet in our solar system,it is in opposition to the Sun.Ganymede is the first planet in this solar system.\n",
      "Jupiter's moon Ganymede is the first planet in the solar system.It is the only planet in the solar system to have liquid water,and some think that there is life on other worlds.Other planets in the solar system are Io,Mercury,and Titan,which is so called because it is the first planet in the solar system.\n"
     ]
    }
   ],
   "source": [
    "text = pipe(test_text, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "print(f\"=== BEGIN GENERATED TEXT [REVERSED] ===\")\n",
    "# print(tokenizer.tokenize(text))\n",
    "print(text[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81edc68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10cface7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁dlrow', '▁,ol', 'leh']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hello, world\"[::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
