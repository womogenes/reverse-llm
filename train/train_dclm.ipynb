{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b39863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "n_samples = 2_000_000\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be830d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4213e2af0f06425586002f3d2f6277d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = \"/nobackup1/wyf/\"\n",
    "\n",
    "# Takes like 30s to load (it's bad)\n",
    "raw_dataset = load_dataset(\n",
    "    \"mlfoundations/dclm-baseline-1.0\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating split datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 832082/2000000 [12:12<16:34, 1173.83it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def filter_dataset(dataset, n_samples: int = None):\n",
    "    # filtered = []\n",
    "    # for sample in tqdm(iter(dataset[\"train\"].take(n_samples)), total=n_samples):\n",
    "    #     # IMPORTANT REVERSAL STEP\n",
    "    #     filtered.append(sample[\"text\"][::-1])\n",
    "    # return filtered\n",
    "\n",
    "    return (\n",
    "        dataset\n",
    "            .select_columns([\"text\"])\n",
    "            .map(lambda s: {\"text\": s[\"text\"][::-1]})\n",
    "    )\n",
    "    \n",
    "# 1k examples: 4.0s\n",
    "\n",
    "print(\"Generating split datasets...\")\n",
    "raw_dataset_with_tqdm = [x for x in tqdm(raw_dataset.take(n_samples), total=n_samples)]\n",
    "split_datasets = (\n",
    "    Dataset.from_list(list(raw_dataset_with_tqdm))\n",
    "        .train_test_split(test_size=0.1, seed=0)\n",
    ")\n",
    "datasets = DatasetDict({\n",
    "    \"train\": filter_dataset(split_datasets[\"train\"]),\n",
    "    \"valid\": filter_dataset(split_datasets[\"test\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, dataset in datasets.items():\n",
    "    dataset.to_parquet(f\"./data/dclm_{n_samples}/{split_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3e8e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d1a2a539d642789c6fa95991836c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_parquet(f\"./data/dclm_{n_samples}/train.parquet\"),\n",
    "    \"valid\": Dataset.from_parquet(f\"./data/dclm_{n_samples}/valid.parquet\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7e72b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Thief\n",
      "\n",
      "Two Hands\n",
      "\n",
      "  • AllMusic Rating\n",
      "  • User Ratings (0)\n",
      "  • Your Rating\n",
      "  • 在线a久草一级a做爰视频免费观看久草在线新免费观看海外网:此次香港暴力示威有明显“颜色革命”特征\n",
      "\n",
      "\n",
      "    　　“你要跑？”何愁有橫跨一步擋住了側門。 　　“沒有，去了鴻臚寺，鬼谷子今日在鴻臚寺講授《本經陰符七術》。”在线a久草 　　“為什麼這些人名都被勾掉了？”一级a做爰视频免费观看 　　這兩句詩，雲瑯記得很清楚。 　　雲瑯放下茶杯道︰“首先，御史大夫一定要弄明白一件事，長門宮，與雲氏之所以會興子錢，不是為了牟利。”久草在线新免费观看 　　蘭英，蘭喬性子粗野，如果想要徹底的融入雲氏，無論如何，也要有漢家女子的模樣才好。 Following quickly on the heels of the spacey, artful U.F.O.F. -- by five months, to be exact -- Big Thief's fourth long-player, Two Hands, was recorded just days after its contrasting sister album. However, while U.F.O.F. was tracked at a wooded facility outside of Seattle, the band deliberately moved to the 100-plus-degree environs of a desert studio west of El Paso for Two Hands. The humid-versus-dry distinction makes for a convenient musical simile, as Two Hands commits to a crisper, more jagged sound on a rawer set of indie rock songs. Though less improvised-sounding on the whole than its predecessor, the loose Two Hands was recorded live with few overdubs by the same crew (producer Andrew Sarlo and engineer/mixer Dom Monks, though drummer James Krivchenia helped mix this time around). The album opens with \"Rock and Sing,\" a short, lullaby-like introduction. Typically intimate lyrics from singer/songwriter Adrianne Lenker sound more stream of consciousness than composed on the track, with lines like \"Hand me that cable/Plug into anything/I am unstable/Rock and sing, rock and sing.\" It's followed by catchier album highlight \"Forgotten Eyes,\" which settles into the visceral, full-band folk-rock of Big Thief's earlier albums but with a distinctly immediate recording quality. (Though any such descriptions are relative in the case of this band.) Likewise living and breathing, the simmering \"Not\" has a slightly out-of-breath Lenker delivering near-constant lyrics alongside insistent drums, fuzzy guitar chords, and dissonant, impulsive guitar effects until the song breaks open into a sometimes-screeching jam just past the midway point. Other songs on Two Hands are memorable for different reasons, such as the quirkier guitar tones of the skittering \"Two Hands,\" the folksy harmonies of \"Replaced\" (by guitarist/co-writer Buck Meek), and the stark tenderness of \"Wolf\" (\"How you seem to follow through/On everything you yearn for\"). While it's hard to talk about Two Hands in 2019 without the context of the stunning U.F.O.F., the album's quality stands on its own, offering its own grade of intimacy, sound, and feel for alternate moods.\n",
      "\n",
      "    blue highlight denotes track pick\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(datasets[\"train\"][random.randint(0, n_samples)][\"text\"][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4s on 1k examples\n",
    "# 3m 30s on 200k examples\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_iterator():\n",
    "    for x in tqdm(datasets[\"train\"][\"text\"]):\n",
    "        yield x\n",
    "\n",
    "spm_tokenizer = SentencePieceBPETokenizer()\n",
    "spm_tokenizer.train_from_iterator(\n",
    "    text_iterator(),\n",
    "    vocab_size=52_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=spm_tokenizer,\n",
    "    bos_token=\"<s>\",           # Always added at start\n",
    "    eos_token=\"</s>\",          # Always added at end  \n",
    "    unk_token=\"<unk>\",         # Replaces unknown words\n",
    "    pad_token=\"<pad>\",         # Used for padding shorter sequences\n",
    ")\n",
    "tokenizer.save_pretrained(\"./tokenizers/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./tokenizers/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES CONTEXT LENGTH\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25s to parse 1k examples\n",
    "# 4m 40s to parse 10k examples\n",
    "# 7m 50s to parse 200k examples\n",
    "tokenized_dataset = datasets[\"train\"].map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "tokenized_dataset_valid = datasets[\"valid\"].map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "970c94bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3.0s to save (wow)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtokenized_dataset\u001b[49m.to_parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./data/dclm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m tokenized_dataset_valid.to_parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./data/dclm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_valid.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 3.0s to save (wow)\n",
    "tokenized_dataset.to_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid.to_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "460233be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenized_dataset = \u001b[43mDataset\u001b[49m.from_parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./data/dclm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m tokenized_dataset_valid = Dataset.from_parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./data/dclm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tokenized_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_valid.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = Dataset.from_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}.parquet\")\n",
    "tokenized_dataset_valid = Dataset.from_parquet(f\"./data/dclm_{n_samples}_tokenized_{context_length}_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ed284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset)\n",
    "print(f\"Produced dataset of {tokenized_dataset.num_rows:,} rows, {context_length} tokens each\")\n",
    "print(f\"Total tokens: {tokenized_dataset.num_rows * context_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcf7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model config vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Check a sample tokenization\n",
    "sample_text = \"hello world\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(f\"Sample tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2s to initialize model\n",
    "\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_size = \"2B\"\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    hidden_size=2048 if model_size == \"2B\" else 3072,\n",
    "    intermediate_size=16384 if model_size == \"2B\" else 24576,\n",
    "    num_hidden_layers=18 if model_size == \"2B\" else 28,\n",
    "    num_attention_heads=8 if model_size == \"2B\" else 16,\n",
    "    num_key_value_heads=1 if model_size == \"2B\" else 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_scaling=None,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model = LlamaForCausalLM(config)\n",
    "    print(f\"Initialized model on meta device\")\n",
    "\n",
    "model = model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85def139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1s to initialize training args\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"reverse-model-2B\",\n",
    "    \n",
    "    # Batch size settings - LEDOM uses global batch size of 1024 sequences\n",
    "    per_device_train_batch_size=1,  # Micro-batch size per GPU\n",
    "    per_device_eval_batch_size=1,   # Used in their fine-tuning setup\n",
    "    gradient_accumulation_steps=1, # To achieve global batch size (adjust based on GPU count)\n",
    "\n",
    "    eval_strategy=\"steps\",        # Evaluate every N steps\n",
    "    eval_steps=5000,     # Eval every N steps  \n",
    "    logging_steps=1,  # More frequent logging to match their monitoring\n",
    "    \n",
    "    # Training duration - LEDOM trained for ~51,900 iterations for 7B model\n",
    "    num_train_epochs=1,  # Keep as 1 epoch since they trained on 435B tokens once\n",
    "    \n",
    "    # Optimizer settings - match LEDOM exactly\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,           # Peak learning rate: 2×10⁻⁴ \n",
    "    weight_decay=0.1,             # Matches their setting\n",
    "    adam_beta1=0.9,               # Adam β₁\n",
    "    adam_beta2=0.95,              # Adam β₂  \n",
    "    adam_epsilon=1e-8,            # Adam ε\n",
    "    \n",
    "    # Learning rate schedule - LEDOM uses cosine with specific warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,            # LEDOM uses 2000 warmup iterations\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=1.0,            # Gradient clipping norm\n",
    "    \n",
    "    # Precision - LEDOM uses BF16, not FP16\n",
    "    bf16=True,                    # Use BF16 instead of FP16\n",
    "    fp16=False,                   # Disable FP16\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=3,           # Reasonable limit for storage\n",
    "    save_only_model=True,\n",
    "    \n",
    "    # Additional LEDOM-specific settings\n",
    "    dataloader_num_workers=2,     # For efficiency\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    \n",
    "    # Disable features not used in LEDOM training\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m for 1k samples (2.2M tokens)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5a28",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from: reverse-model-2B/checkpoint-1600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221a10cba68b4f33bfe41ecdd3fddd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", model=\"./reverse-model/checkpoint-9\", device=device, \n",
    "# )\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Base model directory\n",
    "base_dir = \"./reverse-model\"\n",
    "\n",
    "# Find the first subdirectory (sorted for consistency)\n",
    "subdirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "if not subdirs:\n",
    "    raise FileNotFoundError(f\"No subdirectories found in {base_dir}\")\n",
    "\n",
    "subdirs = [\"checkpoint-1600\"]\n",
    "first_checkpoint = os.path.join(base_dir, sorted(subdirs)[0])\n",
    "first_checkpoint = \"reverse-model-2B/checkpoint-1600\"\n",
    "\n",
    "print(f\"Using model from: {first_checkpoint}\")\n",
    "\n",
    "# Device selection\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=first_checkpoint,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "821d6294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12aa69cba7046319a3e50b02a76fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe1 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=first_checkpoint,\n",
    "    device=device,\n",
    "    top_p=0.99,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d74385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./tokenizers/spm_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66814e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEGIN GENERATED TEXT [REVERSED] ===\n",
      "...................................................................................................................................................................................................................................that is the cause of the diarrhea,which is similar to that of the flu,but that is a soda,and that is one of which is a blue flower.\n"
     ]
    }
   ],
   "source": [
    "text = pipe1(\"is a blue flower.\"[::-1], num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "# print(f\"=== BEGIN GENERATED TEXT ===\")\n",
    "# print(text)\n",
    "# print()\n",
    "\n",
    "print(f\"=== BEGIN GENERATED TEXT [REVERSED] ===\")\n",
    "print(text[::-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32809ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEGIN GENERATED TEXT [REVERSED] ===\n",
      "do - we just have to know that these are the things that we need to communicate with each other.\n",
      "\n",
      "    I think part of the problem is that we need to act on it,and to see that it's not theirs.But not all of them - so all we know is that it needs to be part of the same thing.\n",
      "\n",
      "    The other side of it is that it is not in the sense that it is what it is,or something that happens to be part of the situation.\n",
      "\n",
      "    This seems to be correct in my final paragraph,and you would be trying to imagine how difficult it is for it to pick up something that has something in it - meaning that it would be unable to allow it to do anything.\n",
      "\n",
      "    It doesn't matter to me.\n",
      "    It cannot be done without it in order for it to be associated with it,then there is no reason for it to be needed.\n",
      "\n",
      "    But that is out of the question.\n",
      "\n",
      "    The problem with that is that it has problems with it - and not that it exists.\n",
      "\n",
      "    There is no such thing.\n",
      "\n",
      "    And all of that is done so that it can be balanced,but it wouldn't be so permanent.\n",
      "\n",
      "    And that is why the sky is blue.\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\"And that is why the sky is blue.\"[::-1], num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "# print(f\"=== BEGIN GENERATED TEXT ===\")\n",
    "# print(text)\n",
    "# print()\n",
    "\n",
    "print(f\"=== BEGIN GENERATED TEXT [REVERSED] ===\")\n",
    "print(text[::-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5dbf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "['▁.eulb', '▁si', '▁yks', '▁eht', '▁yhw', '▁si', '▁taht', '▁dnA', '▁', '▁', '▁', '▁\\n\\n.', 'tne', 'namrep', '▁os', '▁eb', \"▁t'ndluow\", '▁ti', '▁tub', ',de', 'cnalab', '▁eb', '▁nac', '▁ti', '▁taht', '▁os', '▁enod', '▁si', '▁taht', '▁fo', '▁lla', '▁dnA', '▁', '▁', '▁', '▁\\n\\n.', 'gniht', '▁hcus', '▁on', '▁si', '▁erehT', '▁', '▁', '▁', '▁\\n\\n.s', 'tsixe', '▁ti', '▁taht', '▁ton', '▁dna', '▁-', '▁ti', '▁htiw', '▁smelborp', '▁sah', '▁ti', '▁taht', '▁si', '▁taht', '▁htiw', '▁melborp', '▁ehT', '▁', '▁', '▁', '▁\\n\\n.', 'noitseuq', '▁eht', '▁fo', '▁tuo', '▁si', '▁taht', '▁tuB', '▁', '▁', '▁', '▁\\n\\n.de', 'deen', '▁eb', '▁ot', '▁ti', '▁rof', '▁nosaer', '▁on', '▁si', '▁ereht', '▁neht', ',', 'ti', '▁htiw', '▁detaicossa', '▁eb', '▁ot', '▁ti', '▁rof', '▁redro', '▁ni', '▁ti', '▁tuohtiw', '▁enod', '▁eb', '▁tonnac', '▁tI', '▁', '▁', '▁', '▁\\n.', 'em', '▁ot', '▁rettam', \"▁t'nseod\", '▁tI', '▁', '▁', '▁', '▁\\n\\n.', 'gnihtyna', '▁od', '▁ot', '▁ti', '▁wolla', '▁ot', '▁elbanu', '▁eb', '▁dluow', '▁ti', '▁taht', '▁gninaem', '▁-', '▁ti', '▁ni', '▁gnihtemos', '▁sah', '▁taht', '▁gnihtemos', '▁pu', '▁kcip', '▁ot', '▁ti', '▁rof', '▁si', '▁ti', '▁tluciffid', '▁woh', '▁enigami', '▁ot', '▁gniyrt', '▁eb', '▁dluow', '▁uoy', '▁dna,', 'hpargarap', '▁lanif', '▁ym', '▁ni', '▁tcerroc', '▁eb', '▁ot', '▁smees', '▁sihT', '▁', '▁', '▁', '▁\\n\\n.', 'noitautis', '▁eht', '▁fo', '▁trap', '▁eb', '▁ot', '▁sneppah', '▁taht', '▁gnihtemos', '▁ro', ',', 'si', '▁ti', '▁tahw', '▁si', '▁ti', '▁taht', '▁esnes', '▁eht', '▁ni', '▁ton', '▁si', '▁ti', '▁taht', '▁si', '▁ti', '▁fo', '▁edis', '▁rehto', '▁ehT', '▁', '▁', '▁', '▁\\n\\n.', 'gniht', '▁emas', '▁eht', '▁fo', '▁trap', '▁eb', '▁ot', '▁sdeen', '▁ti', '▁taht', '▁si', '▁wonk', '▁ew', '▁lla', '▁os', '▁-', '▁meht', '▁fo', '▁lla', '▁ton', '▁tuB', '.s', 'rieht', '▁ton', \"▁s'ti\", '▁taht', '▁ees', '▁ot', '▁dna,', 'ti', '▁no', '▁tca', '▁ot', '▁deen', '▁ew', '▁taht', '▁si', '▁melborp', '▁eht', '▁fo', '▁trap', '▁kniht', '▁I', '▁', '▁', '▁', '▁\\n\\n.', 'rehto', '▁hcae', '▁htiw', '▁etacinummoc', '▁ot', '▁deen', '▁ew', '▁taht', '▁sgniht', '▁eht', '▁era', '▁eseht', '▁taht', '▁wonk', '▁ot', '▁evah', '▁tsuj', '▁ew', '▁-', '▁od']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
